{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2017/09/07 - Longest Flowpath not working. Currently it tracks down the longest tributary flowline....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import arcpy\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arcpy.env.overwriteOutput = True\n",
    "arcpy.env.workspace='F:\\SW023129 Los Pen WMP\\900-GIS\\930-Flood Control\\932 Delineation\\932.1 Carroll Canyon\\932.1.1 East of 15\\QGIS_Sandbox\\GDB.gdb'\n",
    "# catchment = 'catchment'\n",
    "catchment = 'upstream_s24745_catchment'\n",
    "drainageline = 'ArcHydro_DL_Clip'\n",
    "dl_cat_int = 'dl_cat_int_all'\n",
    "\n",
    "cat_usid = 'new_ID_S'\n",
    "cat_dsid = 'new_DwnID_S' #to be filled here\n",
    "dl_usid = 'CatID'\n",
    "dl_dsid = 'DwnCatID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_at(input_data):\n",
    "    #Convert the attribute table of a feature class to pandas database\n",
    "    arcpy.conversion.TableToExcel(Input_Table=input_data,\n",
    "                              Output_Excel_File='Output/_temp_input.xls'\n",
    "                             )\n",
    "    df_at = pd.read_excel('Output/_temp_input.xls')\n",
    "    os.remove('Output/_temp_input.xls')\n",
    "    return df_at\n",
    "\n",
    "def Con_in_cat(df_dl):\n",
    "    df_dl_id_unique = df_dl.drop_duplicates(subset=[dl_usid,dl_dsid])\n",
    "    max_row = 0\n",
    "    ds_of_max = []\n",
    "    sr_us = pd.Series([])\n",
    "    for index,row in df_dl_id_unique.iterrows():\n",
    "        _us_list = trace_upstream(row[dl_usid],df_dl_id_unique)\n",
    "        if _us_list.size == 1:\n",
    "            sr_us = sr_us.append(_us_list.iloc[0])\n",
    "        if _us_list.size>max_row:\n",
    "            max_row = _us_list.size\n",
    "            df_lgst_flpth = _us_list\n",
    "            ds_of_max=_us_list.iloc[-1]\n",
    "    \n",
    "    # convert upstream list and downstream catchment ID into a dataframe\n",
    "    sr_us.reset_index(drop=True, inplace=True)\n",
    "    _df_con_in_cat = pd.DataFrame(sr_us,columns=['us'])\n",
    "    _df_con_in_cat['ds']=ds_of_max[0]\n",
    "    return _df_con_in_cat,df_lgst_flpth\n",
    "\n",
    "def trace_upstream(target_ID, df):\n",
    "    _neighbors = df[df[dl_dsid] == target_ID]\n",
    "    upstream = _neighbors\n",
    "    while not _neighbors.empty:\n",
    "        _neighbors = df[df[dl_dsid].isin(_neighbors[dl_usid])]\n",
    "        upstream = upstream.append(_neighbors, ignore_index=True)\n",
    "    #Comment out the line below if one does not want to include the catchment itself\n",
    "    upstream=upstream.append(df[df[dl_usid]==target_ID],ignore_index=True)\n",
    "    exc_cols=df.columns.difference([dl_usid])\n",
    "    upstream = upstream.drop(exc_cols,1)\n",
    "    return upstream\n",
    "\n",
    "def lgst_flpth(df_flpth,org_shp,catID=cat_usid, dlID=dl_usid):\n",
    "     # Define output variables\n",
    "#     target_ID = df_fpth[catID][0]\n",
    "    temp_input='_temp_input_lgst_flpth'\n",
    "    temp_input_xls='Output/'+temp_input+'.xls'\n",
    "    table_gdb='a_lgst_flpth'\n",
    "    table_csv=filename = 'Output/'+table_gdb+'.csv'\n",
    "    fc_output='lgs_flpth'\n",
    "    \n",
    "    df_flpth.to_csv(table_csv,index=False)\n",
    "    arcpy.conversion.TableToTable(table_csv,\n",
    "                                 arcpy.env.workspace,\n",
    "                                table_gdb)\n",
    "    \n",
    "    #Create a temp duplicate shapefile in prep for join field - avoid modifying the original input\n",
    "    arcpy.conversion.FeatureClassToFeatureClass(org_shp, \n",
    "                                            arcpy.env.workspace, \n",
    "                                            temp_input)\n",
    "    \n",
    "    arcpy.management.JoinField(in_data=temp_input,\n",
    "                          in_field=dlID,\n",
    "                          join_table= table_gdb,\n",
    "                          join_field=dlID)\n",
    "    \n",
    "    expression = dlID+'_1 IS NOT NULL' #Furture improvement: Avoid hard coding the epxression\n",
    "    arcpy.conversion.FeatureClassToFeatureClass(temp_input, \n",
    "                                            arcpy.env.workspace, \n",
    "                                            fc_output,\n",
    "                                           expression)\n",
    "    arcpy.management.Delete(temp_input)\n",
    "    arcpy.management.DeleteField(fc_output,dlID+'_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Running Block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Intersect drainage line by catchment \n",
    "arcpy.analysis.Intersect(in_features=[catchment,drainageline],\n",
    "                         out_feature_class=dl_cat_int,\n",
    "                         join_attributes=\"ALL\"\n",
    "                        )\n",
    "# Import catchment \n",
    "df_cat = read_at(catchment)\n",
    "df_dl = read_at(drainageline)\n",
    "df_dl_int = read_at(dl_cat_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table 1 - df_dl_con ( Global drainage line connectivity correlation)\n",
    "### Table 2 - df_con_in_cats (Drainage line connectivity correlation within each catchment)\n",
    "### Table 3 - df_cat_dl_cor (catchment - d/s drainage line correlation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CarE1017\n",
      "CarE1033\n",
      "CarE1133\n",
      "CarE1093\n",
      "CarE1029\n",
      "CarE1032\n",
      "CarE1086\n",
      "CarE1138\n",
      "CarE1026\n",
      "CarE1015\n",
      "CarE1122\n",
      "CarE1137\n",
      "CarE1128\n",
      "CarE1016\n",
      "CarE1031\n",
      "CarE1025\n",
      "CarE1139\n",
      "CarE1129\n",
      "CarE1018\n",
      "CarE1141\n",
      "CarE1030\n",
      "CarE1088\n",
      "CarE1140\n",
      "CarE1134\n",
      "CarE1087\n",
      "CarE1001\n",
      "CarE1027\n",
      "CarE1028\n",
      "CarE1125\n",
      "CarE1130\n",
      "CarE1105\n",
      "CarE1131\n",
      "      CatID  new_ID_S\n",
      "0    s25243  CarE1017\n",
      "1    s25416  CarE1017\n",
      "2    s25487  CarE1017\n",
      "3    s25519  CarE1017\n",
      "4    s25510  CarE1017\n",
      "5    s25450  CarE1017\n",
      "6    s25365  CarE1017\n",
      "7    s25547  CarE1017\n",
      "8    s25366  CarE1017\n",
      "9    s25425  CarE1017\n",
      "10   s25560  CarE1017\n",
      "11   s25677  CarE1017\n",
      "12   s25569  CarE1017\n",
      "13   s25673  CarE1017\n",
      "14   s25751  CarE1017\n",
      "15   s25844  CarE1017\n",
      "16   s25757  CarE1017\n",
      "17   s25957  CarE1017\n",
      "18   s26035  CarE1017\n",
      "19   s26043  CarE1017\n",
      "20   s25744  CarE1017\n",
      "21   s25769  CarE1017\n",
      "22   s26016  CarE1017\n",
      "23   s26165  CarE1017\n",
      "24   s25740  CarE1017\n",
      "25   s25756  CarE1017\n",
      "26   s25788  CarE1017\n",
      "27   s25737  CarE1017\n",
      "28   s25750  CarE1017\n",
      "29   s25730  CarE1017\n",
      "..      ...       ...\n",
      "92   s24135  CarE1105\n",
      "93   s24377  CarE1105\n",
      "94   s24423  CarE1105\n",
      "95   s24486  CarE1105\n",
      "96   s23846  CarE1105\n",
      "97   s24042  CarE1105\n",
      "98   s24509  CarE1105\n",
      "99   s24594  CarE1105\n",
      "100  s24510  CarE1105\n",
      "101  s24566  CarE1105\n",
      "102  s24580  CarE1105\n",
      "103  s24622  CarE1105\n",
      "104  s24581  CarE1105\n",
      "105  s24630  CarE1105\n",
      "106  s24673  CarE1105\n",
      "107  s24674  CarE1105\n",
      "108  s24675  CarE1105\n",
      "109  s24706  CarE1105\n",
      "110  s24745  CarE1105\n",
      "111  s24765  CarE1105\n",
      "112  s24826  CarE1105\n",
      "113  s24959  CarE1105\n",
      "114  s24684  CarE1105\n",
      "0    s21568  CarE1131\n",
      "1    s21909  CarE1131\n",
      "2    s21177  CarE1131\n",
      "3    s22063  CarE1131\n",
      "4    s22083  CarE1131\n",
      "5    s21514  CarE1131\n",
      "6    s21833  CarE1131\n",
      "\n",
      "[1058 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import drainage line and extract drainage line connectivity correlation\n",
    "df_dl_con = df_dl[[dl_usid,dl_dsid]]\n",
    "df_dl_con.set_index(dl_usid,inplace=True)\n",
    "\n",
    "#Initialize dataframe \n",
    "df_con_in_cats = pd.DataFrame([],columns=['us','ds'])\n",
    "df_cat_dl_cor = pd.DataFrame([],columns=[cat_usid,dl_usid])\n",
    "df_lgst_flpth = pd.DataFrame([],columns=[cat_usid,dl_usid])\n",
    "#Beginning of Loop\n",
    "for index,row in df_cat.iterrows():\n",
    "    # Find all u/s and d/s nodes of drainage line within each catchment\n",
    "    sample_cat = row[cat_usid]\n",
    "    print(sample_cat)\n",
    "    _temp_con_in_cat,_temp_lgst_flpth = Con_in_cat(df_dl_int[df_dl_int[cat_usid]==sample_cat])\n",
    "    _temp_lgst_flpth[cat_usid]=sample_cat\n",
    "    df_con_in_cats = pd.concat([df_con_in_cats,_temp_con_in_cat]) \n",
    "    df_lgst_flpth = pd.concat([df_lgst_flpth,_temp_lgst_flpth]) \n",
    "    # Assign d/s drainage line ID to CatID\n",
    "    df_cat_dl_cor.loc[len(df_cat_dl_cor)]=[sample_cat, _temp_con_in_cat['ds'][0]]\n",
    "#End of Loop\n",
    "\n",
    "df_cat_dl_cor.set_index(dl_usid,inplace=True)\n",
    "df_con_in_cats.set_index('us', inplace = True)\n",
    "df_lgst_flpth.to_clipboard()\n",
    "#Create longest flowpath shapefile based on the list\n",
    "\n",
    "\n",
    "# print(df_con_in_cats[0:3])\n",
    "# print(df_cat_dl_cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lgst_flpth(df_lgst_flpth, drainageline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate final output table based on Tables 1 - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Prepare final output table based on Table 1-3\n",
    "\n",
    "# iteration 1\n",
    "_temp_t12 = df_dl_con.join(df_con_in_cats)\n",
    "_temp_t12['ds'].fillna(_temp_t12[dl_dsid],inplace=True)\n",
    "_temp_t123 = _temp_t12.join(df_cat_dl_cor,on='ds') #this table now correlates upstream dl id with downstream catch ID\n",
    "\n",
    "df_final = df_cat_dl_cor.copy()\n",
    "df_final = df_final.join(_temp_t123,rsuffix = '_ds',how='left')\n",
    "df_final = df_final[df_final[cat_usid]!=df_final[cat_usid+'_ds']]\n",
    "# df_final.to_clipboard()\n",
    "\n",
    "# iteration 2\n",
    "_temp_t122 = _temp_t12.join(df_con_in_cats,on='ds',rsuffix='_ds')\n",
    "_temp_t123 = _temp_t122.join(df_cat_dl_cor,on='ds_ds')\n",
    "df_final2 = df_final.join(_temp_t123,rsuffix = '_ds2',how='left')\n",
    "df_final2 = df_final2.drop_duplicates([cat_usid,cat_usid+'_ds'])\n",
    "df_final2[cat_usid+'_ds'].fillna(df_final2[cat_usid+'_ds2'],inplace=True)\n",
    "\n",
    "df_final2.rename(index=str,columns={cat_usid+'_ds': cat_dsid},inplace=True)\n",
    "#export df_final2 to excel for post-processing \n",
    "df_final2.to_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_con_in_cats_d = pd.DataFrame([],columns=['us','ds'])\n",
    "df_cat_dl_cor_d = pd.DataFrame([],columns=[cat_usid,dl_usid])\n",
    "\n",
    "sample_cat = 'CarE1105'\n",
    "_temp_con_in_cat = Con_in_cat(df_dl_int[df_dl_int[cat_usid]==sample_cat])\n",
    "#     print(_temp_con_in_cat)\n",
    "# df_con_in_cats.append(_temp_con_in_cat)\n",
    "df_con_in_cats_d = pd.concat([df_con_in_cats_d,_temp_con_in_cat]) \n",
    "# Assign one d/s to CatID, and find DwnID based on the drainageline connectivity correlation\n",
    "#Add a new row to the new catchment dataframe, which will contain the catchment connectivity info eventually\n",
    "df_cat_dl_cor_d.loc[len(df_cat_dl_cor)]=[sample_cat, _temp_con_in_cat['ds'][0]]\n",
    "\n",
    "df_cat_dl_cor_d.set_index(dl_usid,inplace=True)\n",
    "df_con_in_cats_d.set_index('us', inplace = True)\n",
    "\n",
    "df_con_in_cats_d = df_con_in_cats_d.join(df_cat_dl_cor_d,on='ds')\n",
    "\n",
    "print(df_con_in_cats_d)\n",
    "print(df_cat_dl_cor_d)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
